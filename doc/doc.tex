%----------------------------------------------------------------------------------------
% Author:      Hongzhang "Steve" Shao
% Affiliation: University of Chicago Booth School of Business
% Date:        Winter 2024
%----------------------------------------------------------------------------------------

\documentclass[10pt]{article}

\usepackage{geometry}   % For page layout settings
\geometry{margin=1in}  % Sets all margins to 1 inch

\usepackage{color}      % For color
\usepackage{xcolor}     % For more colors

\usepackage{listings}   % For code listings
\usepackage{hyperref}   % For hyperlinks
\usepackage{enumitem}   % For customizing lists
\usepackage{graphicx}   % For including images

\usepackage{amsmath}    % For mathematical equations and symbols
\usepackage{amssymb}



%----------------------------------------------------------------------------------------
% Document Information
%----------------------------------------------------------------------------------------
\title{Solving HJB Equation in Multi-Class Queueing Control Systems Using Neural Network-Based BSDE Solver}
\author{Hongzhang ``Steve'' Shao}
\date{\today}



%----------------------------------------------------------------------------------------
% Document Content
%----------------------------------------------------------------------------------------
\begin{document}

\maketitle

\begin{abstract}
This document details my study of Jiequn Han's neural network-based numerical method for solving Hamilton-Jacobi-Bellman (HJB) equations. 
I analyze his code implementation and extend the method to solve the HJB equation arising from a dynamic scheduling problem in multi-class queueing systems. 
The document outlines the high-level approach, presents pseudocode for new implementations, and discusses key technical considerations.
\end{abstract}

\vspace{1em}

\tableofcontents



\pagebreak

%----------------------------------------------------------------------------------------
% Introduction
%----------------------------------------------------------------------------------------
\section{The High-level Plan}


\begin{enumerate}
    \item \textbf{Get Jiequn's Python code up and running: (Completed)}
        \begin{enumerate}
            \item[-] \textbf{Task}: Set up Python environment per Jiequn's requirements. $(\checkmark)$
            \item[-] \textbf{Task}: Clone Jiequn's code. Test it. Add testing code for consistency in later changes. $(\checkmark)$
            \item[-] \textbf{Task}: Add comprehensive comments and docstrings to understand the code better. $(\checkmark)$
            \item[-] \textbf{Task}: Reorganize project files for direct use as Python package or git submodule. $(\checkmark)$
            \item[-] \textbf{Result}: Pushed the updated code to Github: \href{https://github.com/Steve-Shao/DeepBSDE-Package}{DeepBSDE-Package}
            \item[-] \textbf{Observation}: Everything works, except {\color{red}the code can only run on CPU, not GPU}. 
            \item[] \emph{Diagnosis: The code uses \texttt{TensorFlow 2.13} (without GPU support), thus can only run on CPU.}
        \end{enumerate}
    \item \textbf{Get Ebru's Python code up and running:}
    \begin{enumerate}
        \item[-] \textbf{Task}: Test Ebru's Python code under Jiequn's project environment. $(\checkmark)$
        \item[-] \textbf{Task}: Solve Ebru's model class with Jiequn's solver. Check consistency in results. \textbf{\color{red}(pending)}
        \item[-] \textbf{Observation}: With Jiequn's code and CPU, the 2D test problem takes $>$ an hour to run. 
        \item[] It will be much easier to work with the code if it can run on GPU. So I added the next step. 
    \end{enumerate}
    \item \textbf{Get Jiequn's Python code running on GPU: (Failed)}
    \begin{enumerate}
        \item[-] Note: There is another distribution of \texttt{TF 2.13} that have \texttt{CUDA} (Nvidia's GPU firmware) support.
        However, \texttt{TF 2.13} only supports \texttt{Cuda 11.3}, not \texttt{Cuda 12.4} used by Vast.ai, Mercury, etc., by default.
        Downgrading \texttt{Cuda} risks conflicts with other packages, so I avoid this option.
        \item[-] \textbf{Attempt 1}: For GPU, setup an env with latest \texttt{Cuda 12.4}-compatible \texttt{TensorFlow 2.18}. $(\checkmark)$
        \item[] \emph{Result: Code cannot run due to syntax changes in TensorFlow from 2.13 to 2.18.}
        \item[-] \textbf{Attempt 2}: Make minimal possible changes to code for compatiblility with both env. $(\checkmark)$
        \item[] \emph{Result: Code runs on GPU with new env, but the training loss is not improving.}
        \item[-] \textbf{Attempt 3}: To check error in Attempt 2, run new code in original env with \texttt{TF 2.13}. $(\checkmark)$
        \item[] \emph{Result: Code runs on CPU as original. Loss converges correctly. Thus, the changes where correct.}
        \item[-] \textbf{Diagnosis}: Two possibilities:
            \begin{itemize}
                \item[(a)] Compatibility of original code itself with GPU: 
                \item[(b)] Compatibility of original code with new \texttt{TF 2.18}: 
            \end{itemize}
            \item[-] \textbf{Attempt 4}: To determine which is the issue, test new code and new env on CPU. $(\checkmark)$
            \item[] \emph{Result: Code runs on CPU with new env, but the training loss is not improving, same as on GPU.}
            \item[] \emph{So, it's (b).}
            \item[-] \textbf{Attempt 5}: Discuss with colleagues for possible fixes. $(\checkmark)$
        \item[] \emph{Result: There's two possible fixes:}
            \begin{itemize}
                \item[-] (i) rewrite the code for \texttt{TF 2.18}
                \item[-] (ii) rewrite the code with \texttt{PyTorch}
            \end{itemize}
        \item[] \emph{\color{red}Dawei mentioned \texttt{TF} is known for inconsistency over versions. He recommended \texttt{PyTorch}.}
        \end{enumerate}
    \item \textbf{Get Jiequn's Python code running on GPU with \texttt{PyTorch} (with minimal changes possible):}
    \item \textbf{Understand Ebru's model in detail. Put pseudocode in writing. Update model class:}
    \item \textbf{Understand ``tricks'' in Ebru's newer Julia code. Update Python model class accordingly:}
    \item \textbf{Attempt training Ebru's model with Jiequn's solver. Report results:}
    \item \textbf{Iteratively improve training results:}
\end{enumerate}




\end{document}
